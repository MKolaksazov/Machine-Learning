{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <b>M. Kolaksazov</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>SOCIAL MEDIA SENTIMENT</B>\n",
    "<BR/>\n",
    "<br/>A task for analyzing the sentiment of the messages in the social media.\n",
    "Analyzis was carried out, based on the content of words in the individual messages. The probability of occurence of words was calculated, followed by the calculation of the pointwise mutual information, or PMI, giving information about how probable is to find inside the text a combination of two words together. After that, the PMI of words from the text, as well as special words from vocabularies was calculated to find out how \"positive\" or \"negative\" the message is.\n",
    "<br/>\n",
    "<br/>For this purpouse, data from the social media \"Twitter\" was used, as well as from the Internet Movie Data Base (IMDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import json\n",
    "import operator\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from numpy import *\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Importing data from Twitter:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWriteJson():\n",
    "    def __init__(self, json_file = None):\n",
    "        self.json_file = json_file\n",
    "        \n",
    "    def read_from_json(self, json_file):\n",
    "        self.json_file = json_file\n",
    "        with open(json_file) as f:  \n",
    "            data_json = json.load(f)  \n",
    "        data = pd.read_json(data_json, typ='series')\n",
    "        json_to_dataframe = pd.DataFrame(data = data['data'], index = data['index'], columns = data['columns'])\n",
    "        return json_to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Text pre-processing:</b>\n",
    "<br/>\n",
    "<br/>Tokenization, cleaning up the tweets from special symbols, hashtags, @-mentions, short words and stop words.\n",
    "<br/>\n",
    "<br/>There was also a Python algorithm, called sentiment analyzis, which can sort the messages on the basis of their positive or negative sentiment.\n",
    "<br/>\n",
    "<br/>It was necessary to remove all of special symbols, but to preserve the symbols, specific for the language, such as accents, umlaut, etc. In addition, the own personal names, or names of places, such as \"Los Angelis\", \"San Francisco\", that were more likely to be found together were necessary to be removed, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetPreprocess():    \n",
    "    def clean_tweet(self, tweet):\n",
    "        tweet = str(tweet)\n",
    "        return ' '.join(re.sub('([â€¦])|([@#][\\w]*)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', tweet).split()) \n",
    "\n",
    "    def remove_stop_words(self, corpus):  \n",
    "        removed_stop_words = []    \n",
    "        for text in corpus:        \n",
    "            removed_stop_words.append(' '.join([word.lower() for word in text.split() \n",
    "                                                if word not in stopwords.words('english')\n",
    "                                                and not word.startswith('@') # exclude @-mentions\n",
    "                                                and not word.startswith('#') # exclude hashtags\n",
    "                                                and not word.startswith('&') # exclude special symbols\n",
    "                                                and not word[0].isupper() # exclude personal names\n",
    "                                                and len(word) > 2])) # exclude very short words\n",
    "        return removed_stop_words\n",
    "    \n",
    "    def analyze_sentiment(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    # extracts the data from tweets in columns\n",
    "    def tweets_to_data_frame(self, tweets):\n",
    "        df = pd.DataFrame(data = [tweet.text for tweet in tweets], columns = ['tweets'])\n",
    "        df['sentiment'] = np.array([self.analyze_sentiment(tweet) for tweet in df['tweets']])              \n",
    "        return df"
   ]
  },
  {
   "attachments": {
    "latex.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMcAAAAfBAMAAAChRsS3AAAAMFBMVEX///+IiIjv7++3t7ePj4+/v7+vr6/f39+np6f39/eXl5fPz8/Hx8fn5+efn5/X19dnCaf9AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEC0lEQVRIDa1VS4gcVRQ9U2V3dXV1TU1kRLKr0CoDLtLEaAizaXRAVAabQISIi3YCihBIKYKbaIoExJXWgNFF/DRmIRjFDsFPmME0xvhBFxMUDYnG3pggbgp1oZnFeO57r/pTBCvOeKHvO+fe+955v3oNbMxqSUH/rQX560mfLSoqpUUVhXknLiw5VFhRVPCy2a0Ls5+H1vic3Z7uPFM0RmH+TlNRDb12CYiF+hQWcFEIYPdUk3NP5Ti8fGDA3fsMXEStcRBWW6jd0sDWOaup2zFv9cYoyXjEDYf5WsPgv/BG0sD3O4Q+85UGdqiTLwH16d13Rdb2DgNzO7+d3gUzAV2g/a5RcqkzZNXsGB5aO4U2yqHTBxpHCI6ehGeSH7P+MNzjqM0DzmHgMnCAsZzVR7nfGbIt3H5lV+hjVMX5aS2p4mCtG3R1bgsbDv0ZvDuAYK8iO4Fq1tegiq7WflTkapaYIogx49DZcHsEduRFOnvDihr3ROJ9B7yuRTrAhayvQTbLBjYqwqkpq9/LZhvsUxS5HWgRHEAp0clKT4k8DI+7myoRKwbO6Cy9Rh7LBkaR2loP9Q9ON/DLIEqgrmxsIn60Yhtot0XEmodXahsRv4Ef7/nU5A0KGoZLQ5HHMYMrbifEuyNx8DtB8L6J3PJO9KyBtRh48ZvdCTxrsxWqlciAm006Q06bgcfEuiJiNTEZzeNG4AHGB+YyOWJuakitDzwv2MNvATIRp5PdMSLvZAolImVifsfro9yalbO5WUUKnMdCI/KJEeGApRa8faon0c9cV9AaGceIrC4nWmRi07/ZFBfQHIgs/q1XwoOfjEIsq2GJ/sB7UAc/3C6/iWrak4LrWkmtY0RKqLRF5DLQxwS6IsK7SOS6Td55GdEYJfbjEi8s7UFx/rHlBWwVxC+EboS/JsFSjFun9/B1enTBSY/M7ZFn5QmU1iAiJxKFyhEqUpvZ+ZuS0lILT246BhxXwbfwIVq6SF3jIfe7LLDbdDlbVZwiVqjQF3zfciWk/ja33pK1c/pTOOOkuE2wneMN8nJLMuNWVpQigQJB9BPGHkhd7XXlOnwpJIjdX234e2VP5eUd5bKpg1eUODO1xOCFFK+qyLm5KUsWnbflpUeApyVa2r+acGYLeI43KR1yd6mB08wvJvm+5L/nY14+MOATIeEkl8BT64gLutUw4076NWTv/+RvI1busfcEfxVYcUQRL3KHHDvwEXPZXzThuszp835s50p4FrMJRfjyDrnTA28u7l7X0COd5M9KTB0kReR2iSn+g48GFxeryAbc+axvi+Bqiuzl5XcD6/63nRVerjCrWW9rd03Po6odvLzQ/E1Gz6537GG/Q0N4beTE147/l+grSUH1TEH+f0v/A2vP+1eQmLP9AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analyzis of the text by the means of the calculation of pointwise mutual information (PMI):</b>\n",
    "<br/>![latex.png](attachment:latex.png)\n",
    "<br/>It is based on the probability for the individual occurence of two words in text,\n",
    "as well as the mutual co-occurence of the two words.\n",
    "<br/>\n",
    "<br/>For this purpouse, a matrix of co-occurence, containing all possible combinations of words was created.\n",
    "After that, probability and the PMI were calculated.\n",
    "<br/>\n",
    "<br/>To sort the words by their semantic orientation, the PMI of a word \n",
    "and a word from positive and negative vocabularies was calculated. Only two words in each dictionary were used (\"good\", \"bad\", and \"happy\" and \"sad\"), in order to decrease the subjective criteria for differentiation.  \n",
    "<br/>\n",
    "<br/><b>Difficulties</b> for implementation of this algorithm were related to the use of different types of data (e.g.: lists, arrays, dictionaries and \"defaultdict\"). A bug, that was very difficult to eliminate was the mismatch of keys in the different dictionaries. After attention was given to set the dictionary keys, in order to originate from the same source, the bug was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetAnalyzer():    \n",
    "    def co_occurrence(self, tweets, num_words=5):\n",
    "        co_occur = defaultdict(lambda : defaultdict(int))\n",
    "        for tweet in tweets:\n",
    "            tweet = tweet.split()\n",
    "            # Build co-occurrence matrix\n",
    "            for i in range(len(tweet)-1):  \n",
    "                for j in range(i+1, len(tweet)):\n",
    "                    word_1, word_2 = sorted([tweet[i], tweet[j]])                \n",
    "                    if word_1 != word_2:\n",
    "                        co_occur[word_1][word_2] += 1\n",
    "        return co_occur\n",
    "        \n",
    "    def probability_calculate(self, text, co_occur):\n",
    "        p_t = {}\n",
    "        p_t_com = defaultdict(lambda : defaultdict(int))\n",
    "        count_words = Counter(\" \".join(text).split(\" \"))\n",
    "        for key_1, n in count_words.items(): \n",
    "            total_amount = len(text)\n",
    "            p_t[key_1] = n / total_amount\n",
    "            for key_2 in co_occur[key_1]:\n",
    "                p_t_com[key_1][key_2] = co_occur[key_1][key_2] / total_amount\n",
    "        return p_t, p_t_com, count_words\n",
    "    def pmi(self, p_t, p_t_com, co_occur, searched_word=None):\n",
    "        pmi = defaultdict(lambda : defaultdict(int))\n",
    "        for key_1 in p_t:\n",
    "            for key_2 in co_occur[key_1]:\n",
    "                multiplied_probability = p_t[key_1] * p_t[key_2]\n",
    "                pmi[key_1][key_2] = np.log2(p_t_com[key_1][key_2] / multiplied_probability)\n",
    "                \n",
    "        positive_vocab = ['happy','good'\n",
    "            ,'great', 'recommend', 'omg', 'beautiful', 'wow', 'happy', ':)', ':-)']\n",
    "            #, 'like', 'love', 'nice', 'awesome', 'outstanding'\n",
    "            #,'fantastic', 'terrific', 'congratulations', 'win']\n",
    "        negative_vocab = ['sad','bad'\n",
    "            ,'sad', 'cried', 'terrible', 'crap', 'useless', 'hate', ':(', ':-(']        \n",
    "        semantic_orientation = {}\n",
    "        for term, n in p_t.items():\n",
    "            positive_assoc = np.sum([pmi[term][tx] for tx in positive_vocab])\n",
    "            negative_assoc = np.sum([pmi[term][tx] for tx in negative_vocab])\n",
    "            semantic_orientation[term] = positive_assoc - negative_assoc\n",
    "        semantic_sorted = sorted(semantic_orientation.items(),key=operator.itemgetter(1),reverse=True)\n",
    "        top_pos = semantic_sorted[:15]\n",
    "        top_neg = semantic_sorted[-15:]\n",
    "        return top_pos, top_neg \n",
    "        \n",
    "    def calculate_accuracy(self, labels):\n",
    "        #changing the index of the predicted by the means of the clustering analysis to be equal with \n",
    "        #the original sentiment analysis \n",
    "        correct = 0\n",
    "        for i in range(labels.shape[1]):\n",
    "            if labels[1,i]== 1:\n",
    "                labels[1,i]= -1\n",
    "            elif labels[1,i]== -1:\n",
    "                labels[1,i]= 1\n",
    "            elif labels[1,i]== 0:\n",
    "                labels[1,i]= 0      \n",
    "            if labels[0,i]==labels[1,i]:\n",
    "                correct+=1\n",
    "        print(\"Comparing the original algorithm for sentiment analysis \\nand the predicted by the means of the clustering analysis: \\n\", labels)\n",
    "        print(correct/(labels.shape[1])*100, \" % accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sorting of messages in positive and negative categories:</b>\n",
    "<br/>\n",
    "<br/>It was very difficult to find objective criteria for separation of the messages, because the sorting of messages was highly dependend on the words, that were included inside the vocabularies. Furthermore, words can have different meanings, based on the context. Thus the sentiment of the text messages was analyzed by the means of unsupervised learning (cluster analyzis). Afterwards, a comparison between the built-in algorithm for sentiment analysis and the clustering analysis was carried out and the accuracy was calculated. The accuracy of cluster analyzis, carried out on the data set from the IMDB was higher, because the data was already separated in two (positive and negative) categories with equal sizes.\n",
    "<br/>\n",
    "<br/>It was also interesting to perform PMI analyzis on the already separated data by the means of the native Python algorithm. It was found out, that words categorized as positive and negative by PMI were differentiated in the corresponding categories, whereas the so-called \"neutral\" category lacks both positive and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the original algorithm for sentiment analysis \n",
      "and the predicted by the means of the clustering analysis: \n",
      " [[ 1  0  0 -1  1  1  0  0  1  1  0  1  0  0  1  1  1  0  1  0  0  0 -1  0\n",
      "   0  0  1  0  0  1  1  0  1  1  1  0 -1  0  0  1  0  0  1  1  0  0  0  0\n",
      "   1  1  1  1  1  0  0  0  1  0 -1  0  0  0  0  0  0  0  1  0 -1  0  1  0\n",
      "  -1  1 -1  1  0  0  0  1  1  1  1  1  1  0  0  1  1  0  0  1 -1  1  1  0\n",
      "   1  1  0  1  1  0  0  1  0  0  1  1  1  1  0  0  1  0  0  0  1  0  0  0\n",
      "   0  0  0  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1  0  1  1  0  1  1  0  1  1  0  0  0  1  0 -1  1  1  1\n",
      "   1 -1  1  1  0 -1  1  1]\n",
      " [ 1  1  1  1  1  1  1  1  1  1  0  1  1  1  1  1  1  1  1  0  1  1  1  0\n",
      "   1  1  1  1  0  1  1  1  1  1  1  1  1  1  0  1  1  1  1  1  1  1  0  0\n",
      "   1  1  1  1  1  0  1  0  1  1  1  0  0  0  0  1  0  0  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  0  1  0  1  1  1  1  1  0  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1  1  1  1  1  0  1  1  1  0  1  1  0  1  0  1  1  1  1  0\n",
      "   1  1  1  1  1  1  1  1]]\n",
      "63.0  % accuracy\n",
      "\n",
      "\n",
      "NEUTRAL SENTIMENT\n",
      "Top positive words:  [('made', 0), ('difficult', 0), ('decision', 0), ('renew', 0), ('fourth', 0), ('season', 0), ('choice', 0), ('come', 0), ('way', 0), ('harsh', 0), ('tai', 0), ('fuck', 0), ('one', 0), ('fish', 0), ('people', 0)]\n",
      "Top negative words:  [('movie', 0), ('split', 0), ('half', 0), ('officially', 0), ('production', 0), ('things', 0), ('value', 0), ('distributes', 0), ('black', 0), ('work', 0), ('far', 0), ('wide', 0), ('190', 0), ('countries', 0), ('get', 0)]\n",
      "\n",
      "\n",
      "NEGATIVE SENTIMENT\n",
      "Top positive words:  [('100', 6.169925001442312), ('class', 6.169925001442312), ('morning', 6.169925001442312), ('finis', 6.169925001442312), ('thank', 0), ('bringing', 0), ('series', 0), ('back', 0), ('television', 0), ('amp', 0), ('felt', 0), ('soul', 0), ('completed', 0), ('means', 0), ('baby', 0)]\n",
      "Top negative words:  [('believe', 0), ('download', 0), ('days', 0), ('sorry', 0), ('better', 0), ('future', 0), ('dont', 0), ('shot', 0), ('one', -9.169925001442312), ('binged', -12.339850002884624), ('never', -12.339850002884624), ('done', -12.339850002884624), ('parts', -12.339850002884624), ('hilarious', -12.339850002884624), ('crushingly', -12.339850002884624)]\n",
      "\n",
      "\n",
      "POSITIVE SENTIMENT\n",
      "Top positive words:  [('boy', 10.132178380915544), ('netflix', 5.066089190457772), ('cry', 5.066089190457772), ('laugh', 5.066089190457772), ('harnessed', 5.066089190457772), ('watched', 4.481126689736616), ('last', 4.481126689736616), ('night', 4.481126689736616), ('basically', 4.481126689736616), ('hours', 4.481126689736616), ('stomach', 4.481126689736616), ('slowly', 4.481126689736616), ('dropping', 4.481126689736616), ('hand', 4.481126689736616), ('phil', 4.481126689736616)]\n",
      "Top negative words:  [('absolutely', 0), ('away', 0), ('tear', 0), ('eye', 0), ('better', 0), ('personifies', 0), ('resilience', 0), ('human', 0), ('spirit', 0), ('power', 0), ('overcome', 0), ('hardships', 0), ('thinking', 0), ('person', 0), ('ought', 0)]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    read_write_json = ReadWriteJson()\n",
    "    tweet_analyzer = TweetAnalyzer()\n",
    "    tweet_preprocess = TweetPreprocess()\n",
    "    json_file = 'D:\\\\Marko\\\\ML\\\\coding examples\\\\twitter_data.json'\n",
    "    df = read_write_json.read_from_json(json_file) \n",
    "    text = [tweet_preprocess.clean_tweet(line) for line in df['tweets']]\n",
    "    text = tweet_preprocess.remove_stop_words(text)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer() \n",
    "    tfidf_vectorizer.fit(text)\n",
    "    reviews_features = tfidf_vectorizer.transform(text) \n",
    "    labels = df['sentiment']\n",
    "    \n",
    "    #62.5 % on the dataset with from twitter (3 groups)\n",
    "    k_means = DBSCAN(eps = 0.99, min_samples = 10)\n",
    "    #86.875 % on the dataset from IMDB (2 groups)\n",
    "    #k_means = KMeans(n_clusters = 2, init = 'k-means++') \n",
    "\n",
    "    assigned = k_means.fit_predict(reviews_features)\n",
    "    labels = np.vstack((labels, assigned))\n",
    "    tweet_analyzer.calculate_accuracy(labels)\n",
    "              \n",
    "    for i in range(3):\n",
    "        dat = df[df.sentiment == i-1]\n",
    "        text = [tweet_preprocess.clean_tweet(line) for line in dat['tweets']]\n",
    "        text = tweet_preprocess.remove_stop_words(text)\n",
    "        co_occur = tweet_analyzer.co_occurrence(text)\n",
    "        p_t, p_t_com, count_words = tweet_analyzer.probability_calculate(text, co_occur)\n",
    "        top_pos, top_neg = tweet_analyzer.pmi(p_t, p_t_com, co_occur) \n",
    "        if i == 0:\n",
    "            print('\\n\\nNEUTRAL SENTIMENT')\n",
    "        elif i == 1:\n",
    "            print('\\n\\nNEGATIVE SENTIMENT')\n",
    "        elif i == 2:\n",
    "            print('\\n\\nPOSITIVE SENTIMENT')\n",
    "        print(\"Top positive words: \", top_pos)\n",
    "        print(\"Top negative words: \", top_neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
